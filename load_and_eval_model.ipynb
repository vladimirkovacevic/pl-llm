{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
    "import torch.nn.functional as F\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset \n",
    "ds = datasets.load_from_disk(\"binding_ds_200k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data collator\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "esm_model_name = \"facebook/esm2_t33_650M_UR50D\"  # Replace with the correct ESM2 model name\n",
    "chem_model_name = \"seyonec/ChemBERTa-zinc-base-v1\" # Replace with the correct ChemLLM model name\n",
    "\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, chem_collator, esm_collator):\n",
    "            self.chem_collator = chem_collator\n",
    "            self.esm_collator = esm_collator\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch_ligand = [{\"input_ids\": b[\"ligand_input_ids\"], \"attention_mask\": b[\"ligand_attention_mask\"]} for b in batch]\n",
    "        batch_protein = [{\"input_ids\": b[\"protein_input_ids\"], \"attention_mask\": b[\"protein_attention_mask\"]} for b in batch]\n",
    "\n",
    "        collated_chem = self.chem_collator(batch_ligand)\n",
    "        collated_esm = self.esm_collator(batch_protein)\n",
    "\n",
    "        return {\n",
    "            \"ligand_input_ids\": collated_chem[\"input_ids\"],\n",
    "            \"ligand_attention_mask\": collated_chem[\"attention_mask\"],\n",
    "            \"protein_input_ids\": collated_esm[\"input_ids\"],\n",
    "            \"protein_attention_mask\": collated_esm[\"attention_mask\"],\n",
    "            \"ic50\": torch.tensor([x['ic50'] for x in batch])\n",
    "        }\n",
    "\n",
    "chem_tokenizer = AutoTokenizer.from_pretrained(chem_model_name)\n",
    "esm_tokenizer = AutoTokenizer.from_pretrained(esm_model_name)\n",
    "chem_collator = DataCollatorWithPadding(tokenizer=chem_tokenizer)\n",
    "esm_collator = DataCollatorWithPadding(tokenizer=esm_tokenizer)\n",
    "collator = CustomDataCollator(chem_collator=chem_collator, esm_collator=esm_collator)\n",
    "bs=32\n",
    "test_dataloader = DataLoader(ds['test'], batch_size=bs, collate_fn=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "WARMUP_STEPS = 5000\n",
    "EPOCHS = 30\n",
    "SAMPLE_SIZE = 200_000\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_heads=8, dropout=0.1, ffn_hidden_dim=2048):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.protein_to_ligand_attention = nn.MultiheadAttention(\n",
    "            embed_dim, num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.ligand_to_protein_attention = nn.MultiheadAttention(\n",
    "            embed_dim, num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        ffn_hidden_dim = embed_dim * 3\n",
    "        self.dropout = dropout\n",
    "        self.ffn_protein = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_hidden_dim),\n",
    "            nn.GELU(),  # Non-linear activation\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ffn_hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.ffn_ligand = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_hidden_dim),\n",
    "            nn.GELU,  # Non-linear activation\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ffn_hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.protein_norm = nn.LayerNorm(embed_dim)\n",
    "        self.ligand_norm = nn.LayerNorm(embed_dim)\n",
    "        self.ffn_protein_norm = nn.LayerNorm(embed_dim)\n",
    "        self.ffn_ligand_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        protein_embedding,\n",
    "        ligand_embedding,\n",
    "        key_pad_mask_prot,\n",
    "        key_pad_mask_ligand,\n",
    "    ):\n",
    "        # Protein attending to ligand\n",
    "        attended_protein, _ = self.protein_to_ligand_attention(\n",
    "            query=protein_embedding,\n",
    "            key=ligand_embedding,\n",
    "            value=ligand_embedding,\n",
    "            key_padding_mask=key_pad_mask_ligand,\n",
    "        )\n",
    "        attended_protein = self.protein_norm(\n",
    "            protein_embedding + nn.Dropout(attended_protein)\n",
    "        )  # Residual connection\n",
    "        x_prot = self.ffn_protein(attended_protein)\n",
    "        x_prot = self.ffn_protein_norm(attended_protein + nn.Dropout(x_prot))\n",
    "\n",
    "        # Ligand attending to protein\n",
    "        attended_ligand, _ = self.ligand_to_protein_attention(\n",
    "            query=ligand_embedding,\n",
    "            key=protein_embedding,\n",
    "            value=protein_embedding,\n",
    "            key_padding_mask=key_pad_mask_prot,\n",
    "        )\n",
    "        attended_ligand = self.ligand_norm(\n",
    "            ligand_embedding + attended_ligand\n",
    "        )  # Residual connection\n",
    "        x_ligand = self.ffn_ligand(attended_ligand)\n",
    "        x_ligand = self.ffn_ligand_norm(attended_ligand + nn.Dropout(x_ligand))\n",
    "        return x_prot, x_ligand\n",
    "\n",
    "\n",
    "class BindingAffinityModelWithMultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, esm_model_name, chem_model_name, num_layers=NUM_LAYERS, hidden_dim=1024, dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dropout = dropout\n",
    "        # Load pretrained ESM2 model for proteins\n",
    "        self.esm_model = AutoModel.from_pretrained(esm_model_name)\n",
    "        self.esm_tokenizer = AutoTokenizer.from_pretrained(esm_model_name)\n",
    "        self.esm_model.eval()\n",
    "\n",
    "        # Load pretrained ChemLLM for SMILES (ligands)\n",
    "        self.ligand_model = AutoModel.from_pretrained(chem_model_name)\n",
    "        self.ligand_tokenizer = AutoTokenizer.from_pretrained(chem_model_name)\n",
    "        self.ligand_model.eval()\n",
    "\n",
    "        # Disable gradient computation for both base models\n",
    "        for param in self.esm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.ligand_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Protein and SMILES embedding dimensions\n",
    "        self.protein_embedding_dim = self.esm_model.config.hidden_size\n",
    "        self.ligand_embedding_dim = self.ligand_model.config.hidden_size\n",
    "\n",
    "        self.project_to_common = nn.Linear(\n",
    "            self.protein_embedding_dim, self.ligand_embedding_dim\n",
    "        )\n",
    "        # self.project_to_common = nn.ModuleList([\n",
    "        #     nn.Linear(self.protein_embedding_dim, self.ligand_embedding_dim)\n",
    "        #     for _ in range(num_layers)\n",
    "        # ])\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                CrossAttentionLayer(embed_dim=self.ligand_embedding_dim)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.final_norm = nn.LayerNorm(self.protein_embedding_dim + self.ligand_embedding_dim)\n",
    "\n",
    "        self.ffn_ic50 = nn.Sequential(\n",
    "            nn.Linear(2 * self.ligand_embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        ligand_input_ids,\n",
    "        ligand_attention_mask,\n",
    "        protein_input_ids,\n",
    "        protein_attention_mask,\n",
    "    ):\n",
    "        # Protein embedding\n",
    "        # protein_inputs = self.esm_tokenizer(protein_sequence, return_tensors=\"pt\")\n",
    "        protein_inputs = {\n",
    "            \"input_ids\": protein_input_ids,\n",
    "            \"attention_mask\": protein_attention_mask,\n",
    "        }\n",
    "        # perform in FP16 for lower memory usage (matmuls)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                protein_outputs = self.esm_model(**protein_inputs)\n",
    "        special_tokens_mask_prot = (\n",
    "            (protein_inputs[\"input_ids\"] == self.esm_tokenizer.cls_token_id)\n",
    "            | (protein_inputs[\"input_ids\"] == self.esm_tokenizer.eos_token_id)\n",
    "            | (protein_inputs[\"input_ids\"] == self.esm_tokenizer.pad_token_id)\n",
    "        )\n",
    "        protein_embedding = protein_outputs.last_hidden_state\n",
    "\n",
    "        # SMILES embedding\n",
    "        ligand_inputs = {\n",
    "            \"input_ids\": ligand_input_ids,\n",
    "            \"attention_mask\": ligand_attention_mask,\n",
    "        }\n",
    "        special_tokens_mask_ligand = (\n",
    "            (ligand_inputs[\"input_ids\"] == self.ligand_tokenizer.bos_token_id)\n",
    "            | (ligand_inputs[\"input_ids\"] == self.ligand_tokenizer.eos_token_id)\n",
    "            | (ligand_inputs[\"input_ids\"] == self.ligand_tokenizer.pad_token_id)\n",
    "        )\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                ligand_outputs = self.ligand_model(**ligand_inputs)\n",
    "        ligand_embedding = ligand_outputs.last_hidden_state\n",
    "\n",
    "        # project embeddings to same dimension\n",
    "        protein_embedding = self.project_to_common(protein_embedding)\n",
    "\n",
    "        for i, layer in enumerate(self.layers): # Adding skip-connections\n",
    "            protein_embedding, ligand_embedding = layer(\n",
    "                protein_embedding,\n",
    "                ligand_embedding,\n",
    "                special_tokens_mask_prot,\n",
    "                special_tokens_mask_ligand,\n",
    "            )\n",
    "            if i % 2 == 1: # Every second layer add skip-connection\n",
    "                protein_embedding = protein_embedding + prev_protein_embedding\n",
    "                ligand_embedding = ligand_embedding + prev_ligand_embedding\n",
    "            prev_protein_embedding, prev_ligand_embedding = protein_embedding, ligand_embedding\n",
    "\n",
    "\n",
    "        # Perform mean pooling\n",
    "        ligand_embedding = (\n",
    "            ligand_embedding * ~special_tokens_mask_ligand.unsqueeze(dim=-1)\n",
    "        ).mean(dim=1)\n",
    "        protein_embedding = (\n",
    "            protein_embedding * ~special_tokens_mask_prot.unsqueeze(dim=-1)\n",
    "        ).mean(dim=1)\n",
    "        # Combine embeddings\n",
    "        combined = torch.cat([protein_embedding, ligand_embedding], dim=1)\n",
    "        combined = self.final_norm(combined)\n",
    "        ic50_prediction = self.ffn_ic50(combined)\n",
    "        return ic50_prediction\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BindingAffinityModelWithMultiHeadCrossAttention(esm_model_name, chem_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        test_progress = tqdm(test_loader, desc=\"Test set\")\n",
    "        with torch.no_grad():\n",
    "            for batch in test_progress:\n",
    "                ligand_input_ids = batch[\"ligand_input_ids\"].to(device)\n",
    "                ligand_attention_mask = batch[\"ligand_attention_mask\"].to(device)\n",
    "                protein_input_ids = batch[\"protein_input_ids\"].to(device)\n",
    "                protein_attention_mask = batch[\"protein_attention_mask\"].to(device)\n",
    "                targets = batch[\"ic50\"].unsqueeze(dim=-1).to(device)\n",
    "                preds = model(\n",
    "                    ligand_input_ids,\n",
    "                    ligand_attention_mask,\n",
    "                    protein_input_ids,\n",
    "                    protein_attention_mask,\n",
    "                )\n",
    "                all_targets.append(targets)\n",
    "                all_predictions.append(preds)\n",
    "\n",
    "        all_predictions = torch.cat(all_predictions)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "\n",
    "        loss = torch.nn.MSELoss()\n",
    "        print(\n",
    "            f\"Test set mean squared error (MSE): {loss(all_predictions, all_targets)}\"\n",
    "        )\n",
    "\n",
    "        mse = mean_squared_error(all_targets.cpu(), all_predictions.cpu())\n",
    "        mae = mean_absolute_error(all_targets.cpu(), all_predictions.cpu())\n",
    "        r2 = r2_score(all_targets.cpu(), all_predictions.cpu())\n",
    "        print(f\"MSE: {mse}, MAE: {mae}, R²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"affinity_03_02_2025_16_28_20_24.pt\")\n",
    "# checkpoint = torch.load(\"/root/finetuning/affinity_21_01_2025_23_00_56_26.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test set: 100%|██████████| 260/260 [09:14<00:00,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set mean squared error (MSE): 6.241326808929443\n",
      "MSE: 6.241326808929443, MAE: 1.8755537271499634, R²: 0.4396736217242718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
