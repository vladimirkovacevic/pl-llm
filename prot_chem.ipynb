{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6801bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikolamilicevic\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6fa6443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/finetuning/wandb/run-20250116_164646-jbjdpf0l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nikolamilicevic/affinity/runs/jbjdpf0l' target=\"_blank\">testing mha cross blocks</a></strong> to <a href='https://wandb.ai/nikolamilicevic/affinity' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nikolamilicevic/affinity' target=\"_blank\">https://wandb.ai/nikolamilicevic/affinity</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nikolamilicevic/affinity/runs/jbjdpf0l' target=\"_blank\">https://wandb.ai/nikolamilicevic/affinity/runs/jbjdpf0l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/nikolamilicevic/affinity/runs/jbjdpf0l?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fcd72f479a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"affinity\",\n",
    "    name=\"testing mha cross blocks\",\n",
    "    config={\n",
    "        \"batch_size\": 8,\n",
    "        \"dataset\": \"testing_200k\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1df91290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# Created with csvcut -t -c \"Ligand SMILES,BindingDB Target Chain Sequence,IC50 (nM)\" BindingDB_All.tsv > BindingDB_fin.csv\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"BindingDB_fin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4e5f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ligand SMILES</th>\n",
       "      <th>BindingDB Target Chain Sequence</th>\n",
       "      <th>IC50 (nM)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O[C@@H]1[C@@H](O)[C@@H](Cc2ccccc2)N(CCCCCC(O)=...</td>\n",
       "      <td>PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKM...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O[C@@H]1[C@@H](O)[C@@H](Cc2ccccc2)N(C\\C=C\\c2cn...</td>\n",
       "      <td>PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKM...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O[C@@H]1[C@@H](O)[C@@H](Cc2ccccc2)N(CC2CC2)C(=...</td>\n",
       "      <td>PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKM...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OCCCCCCN1[C@H](Cc2ccccc2)[C@H](O)[C@@H](O)[C@@...</td>\n",
       "      <td>PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKM...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OCCCCCN1[C@H](Cc2ccccc2)[C@H](O)[C@@H](O)[C@@H...</td>\n",
       "      <td>PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKM...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937446</th>\n",
       "      <td>CC[C@H]1CN(Cc2cc(C)cc(CC(O)=O)c2)CCN1c1nc2ccc(...</td>\n",
       "      <td>MGETLGDSPVDPEHGAFADALPMSTSQEITMVDTEMPFWPTNFGIS...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937447</th>\n",
       "      <td>CC[C@@H]1CN(Cc2cc(C)cc(CC(O)=O)c2)CCN1c1nc2ccc...</td>\n",
       "      <td>MGETLGDSPVDPEHGAFADALPMSTSQEITMVDTEMPFWPTNFGIS...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937448</th>\n",
       "      <td>CC(C)[C@@H]1CN(Cc2cc(C)cc(CC(O)=O)c2)CCN1c1nc2...</td>\n",
       "      <td>MGETLGDSPVDPEHGAFADALPMSTSQEITMVDTEMPFWPTNFGIS...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937449</th>\n",
       "      <td>COc1ccc(cc1)N(C)c1nc(C)nc2[nH]ccc12</td>\n",
       "      <td>CVSASPSTLARLVSRSAMPAGSSTAWNTAFSPMARCQVTKTIGGGD...</td>\n",
       "      <td>2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937450</th>\n",
       "      <td>COc1ccc(\\C=C/c2cc(OC)c(OC)c(OC)c2)cc1O</td>\n",
       "      <td>CVSASPSTLARLVSRSAMPAGSSTAWNTAFSPMARCQVTKTIGGGD...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2937451 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Ligand SMILES  \\\n",
       "0        O[C@@H]1[C@@H](O)[C@@H](Cc2ccccc2)N(CCCCCC(O)=...   \n",
       "1        O[C@@H]1[C@@H](O)[C@@H](Cc2ccccc2)N(C\\C=C\\c2cn...   \n",
       "2        O[C@@H]1[C@@H](O)[C@@H](Cc2ccccc2)N(CC2CC2)C(=...   \n",
       "3        OCCCCCCN1[C@H](Cc2ccccc2)[C@H](O)[C@@H](O)[C@@...   \n",
       "4        OCCCCCN1[C@H](Cc2ccccc2)[C@H](O)[C@@H](O)[C@@H...   \n",
       "...                                                    ...   \n",
       "2937446  CC[C@H]1CN(Cc2cc(C)cc(CC(O)=O)c2)CCN1c1nc2ccc(...   \n",
       "2937447  CC[C@@H]1CN(Cc2cc(C)cc(CC(O)=O)c2)CCN1c1nc2ccc...   \n",
       "2937448  CC(C)[C@@H]1CN(Cc2cc(C)cc(CC(O)=O)c2)CCN1c1nc2...   \n",
       "2937449                COc1ccc(cc1)N(C)c1nc(C)nc2[nH]ccc12   \n",
       "2937450             COc1ccc(\\C=C/c2cc(OC)c(OC)c(OC)c2)cc1O   \n",
       "\n",
       "                           BindingDB Target Chain Sequence IC50 (nM)  \n",
       "0        PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKM...       NaN  \n",
       "1        PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKM...       NaN  \n",
       "2        PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKM...       NaN  \n",
       "3        PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKM...       NaN  \n",
       "4        PQITLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKM...       NaN  \n",
       "...                                                    ...       ...  \n",
       "2937446  MGETLGDSPVDPEHGAFADALPMSTSQEITMVDTEMPFWPTNFGIS...       NaN  \n",
       "2937447  MGETLGDSPVDPEHGAFADALPMSTSQEITMVDTEMPFWPTNFGIS...       NaN  \n",
       "2937448  MGETLGDSPVDPEHGAFADALPMSTSQEITMVDTEMPFWPTNFGIS...       NaN  \n",
       "2937449  CVSASPSTLARLVSRSAMPAGSSTAWNTAFSPMARCQVTKTIGGGD...      2600  \n",
       "2937450  CVSASPSTLARLVSRSAMPAGSSTAWNTAFSPMARCQVTKTIGGGD...      1000  \n",
       "\n",
       "[2937451 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687c0500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TESTING PURPOSES REDUCE DATASET SIZE\n",
    "df = df.sample(n=10_000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8e5110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = df.rename(columns={\"Ligand SMILES\": \"ligand\", \"BindingDB Target Chain Sequence\": \"protein\", \"IC50 (nM)\": \"ic50\"})\n",
    "ic50 = df['ic50'][~df['ic50'].isna()]\n",
    "less_than = ic50.str.contains(\"<\")\n",
    "greater_than = ic50.str.contains(\">\")\n",
    "ic50 = ic50[~(less_than | greater_than)]\n",
    "ic50n = ic50.astype(float)\n",
    "df = df.loc[ic50n.index]\n",
    "#  IC50 values are logarithmic in nature;\n",
    "# a compound with an IC50 of 10 nM is 10x more potent than one with 100 nM\n",
    "df['ic50'] = ic50n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26124b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['ic50'] != 0.0]\n",
    "df.loc[:, 'ic50'] = np.log(df['ic50'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fb4314a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ligands total: 1562952\n",
      "Ligands unique 819232\n",
      "Proteins total 1562952\n",
      "Proteins unique 6475\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ligands total: {len(df['ligand'].values)}\")\n",
    "print(f\"Ligands unique {len(set(df['ligand'].values))}\")\n",
    "print(f\"Proteins total {len(df['protein'].values)}\")\n",
    "print(f\"Proteins unique {len(set(df['protein'].values))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9cd9921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.5597, dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BASELINE DUMMY MODEL \n",
    "# that predicts mean of ic50\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "mean_ic50 = np.mean(df['ic50'])\n",
    "y_pred_dummy = np.full_like(df['ic50'], mean_ic50)\n",
    "loss = nn.MSELoss()\n",
    "loss(torch.tensor(y_pred_dummy), torch.tensor(df['ic50'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3df6f13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ligand', 'protein', 'ic50'],\n",
       "    num_rows: 5339\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "dataset = datasets.Dataset.from_pandas(df.reset_index(drop=True))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50fdf4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ic50n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a6e203b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='ic50', ylabel='Count'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# sns.histplot(df['ic50'], bins=100, color='blue')\n",
    "# sns.histplot(ic50n, bins=100, color='blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbe73366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# chem_tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "# chem_model = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "# esm_model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
    "# esm_tokenizer = AutoTokenizer.from_pretrained(esm_model_name)\n",
    "# esm_model = AutoModel.from_pretrained(esm_model_name)\n",
    "# seyonec/ChemBERTa-zinc250k-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b178d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.memory_reserved()/(1024*1024))\n",
    "# print(torch.cuda.memory_allocated()/(1024*1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bacbb6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esm_tokenizer.decode(esm_input['input_ids'][0])\n",
    "# esm_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6bfb3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chem_tokenizer.decode(chem_inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc2ed9d0-cdd4-4984-822f-c0fe13d0372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
    "import torch.nn.functional as F\n",
    "\n",
    "    \n",
    "class BindingAffinityModel(nn.Module):\n",
    "    def __init__(self, esm_model_name, chem_model_name, combined_embedding_dim=512):\n",
    "        super().__init__()\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load pretrained ESM2 model for proteins\n",
    "        self.esm_model = AutoModel.from_pretrained(esm_model_name)\n",
    "        self.esm_tokenizer = AutoTokenizer.from_pretrained(esm_model_name)\n",
    "        self.esm_model.eval()\n",
    "        \n",
    "        # Load pretrained ChemLLM for SMILES (ligands)\n",
    "        self.chem_model = AutoModel.from_pretrained(chem_model_name)\n",
    "        self.chem_tokenizer = AutoTokenizer.from_pretrained(chem_model_name)\n",
    "        self.chem_model.eval()\n",
    "\n",
    "        # Disable gradient computation for both base models\n",
    "        for param in self.esm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.chem_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Protein and SMILES embedding dimensions\n",
    "        self.protein_embedding_dim = self.esm_model.config.hidden_size\n",
    "        self.chem_embedding_dim = self.chem_model.config.hidden_size\n",
    "        \n",
    "        # Projection layers to ensure same dimensionality\n",
    "        # self.protein_projector = nn.Linear(self.protein_embedding_dim, combined_embedding_dim)\n",
    "        # self.chem_projector = nn.Linear(self.chem_embedding_dim, combined_embedding_dim)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        # self.combined_embedding_dim = combined_embedding_dim * 2\n",
    "        self.fc_combine = nn.Linear(self.protein_embedding_dim + self.chem_embedding_dim, combined_embedding_dim)\n",
    "        # self.fc_additional = nn.Linear(combined_embedding_dim, combined_embedding_dim)\n",
    "        self.fc_out = nn.Linear(combined_embedding_dim, 1)  # Predict IC50 as a single value\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            ligand_input_ids,\n",
    "            ligand_attention_mask,\n",
    "            protein_input_ids,\n",
    "            protein_attention_mask\n",
    "        ):\n",
    "        # Protein embedding\n",
    "        # protein_inputs = self.esm_tokenizer(protein_sequence, return_tensors=\"pt\")\n",
    "        protein_inputs = {\n",
    "            \"input_ids\": protein_input_ids,\n",
    "            \"attention_mask\": protein_attention_mask\n",
    "        }\n",
    "        # perform in FP16 for lower memory usage (matmuls)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                protein_outputs = self.esm_model(**protein_inputs)\n",
    "        special_tokens_mask = (protein_inputs['input_ids'] == self.esm_tokenizer.cls_token_id)\\\n",
    "        | (protein_inputs['input_ids'] == self.esm_tokenizer.eos_token_id)\\\n",
    "        | (protein_inputs['input_ids'] == self.esm_tokenizer.pad_token_id)\n",
    "        valid_tokens_mask = ~special_tokens_mask\n",
    "        protein_embedding = (protein_outputs.last_hidden_state * valid_tokens_mask.unsqueeze(dim=-1)).mean(dim=1)\n",
    "        # protein_embedding = self.protein_projector(protein_embedding)  # Project to combined_embedding_dim\n",
    "        \n",
    "        # SMILES embedding\n",
    "        # chem_inputs = self.chem_tokenizer(ligand_smiles, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        chem_inputs = {\n",
    "            \"input_ids\": ligand_input_ids,\n",
    "            \"attention_mask\": ligand_attention_mask\n",
    "        }\n",
    "        special_tokens_mask = (chem_inputs['input_ids'] == self.chem_tokenizer.bos_token_id)\\\n",
    "        | (chem_inputs['input_ids'] == self.chem_tokenizer.eos_token_id)\\\n",
    "        | (chem_inputs['input_ids'] == self.chem_tokenizer.pad_token_id)\n",
    "        valid_tokens_mask = ~special_tokens_mask\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                chem_outputs = self.chem_model(**chem_inputs)\n",
    "        chem_embedding = (chem_outputs.last_hidden_state * valid_tokens_mask.unsqueeze(dim=-1)).mean(dim=1)\n",
    "        # chem_embedding = self.chem_projector(chem_embedding)  # Project to combined_embedding_dim\n",
    "        \n",
    "        # Combine embeddings (already projected)\n",
    "        x = torch.cat([protein_embedding, chem_embedding], dim=1)\n",
    "        x = F.relu(self.fc_combine(x))\n",
    "        # x = F.relu(self.fc_additional(x))\n",
    "        # Output layer\n",
    "        binding_affinity = self.fc_out(x)\n",
    "        return binding_affinity\n",
    "\n",
    "class BindingAffinityModelWithAttention(nn.Module):\n",
    "    def __init__(self, esm_model_name, chem_model_name, combined_embedding_dim=512, attention_dim=256):\n",
    "        super().__init__()\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load pretrained ESM2 model for proteins\n",
    "        self.esm_model = AutoModel.from_pretrained(esm_model_name)\n",
    "        self.esm_tokenizer = AutoTokenizer.from_pretrained(esm_model_name)\n",
    "        self.esm_model.eval()\n",
    "        \n",
    "        # Load pretrained ChemLLM for SMILES (ligands)\n",
    "        self.chem_model = AutoModel.from_pretrained(chem_model_name)\n",
    "        self.chem_tokenizer = AutoTokenizer.from_pretrained(chem_model_name)\n",
    "        self.chem_model.eval()\n",
    "\n",
    "        # Disable gradient computation for both base models\n",
    "        for param in self.esm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.chem_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Protein and SMILES embedding dimensions\n",
    "        self.protein_embedding_dim = self.esm_model.config.hidden_size\n",
    "        self.chem_embedding_dim = self.chem_model.config.hidden_size\n",
    "        \n",
    "        # Linear layers to process individual embeddings\n",
    "        self.protein_linear = nn.Linear(self.protein_embedding_dim, attention_dim)\n",
    "        self.ligand_linear = nn.Linear(self.chem_embedding_dim, attention_dim)\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = nn.Linear(attention_dim, 1)\n",
    "        \n",
    "        # Combined processing layers\n",
    "        self.combined_linear = nn.Linear(self.protein_embedding_dim + self.chem_embedding_dim, combined_embedding_dim)\n",
    "        self.output_layer = nn.Linear(combined_embedding_dim, 1)\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            ligand_input_ids,\n",
    "            ligand_attention_mask,\n",
    "            protein_input_ids,\n",
    "            protein_attention_mask\n",
    "        ):\n",
    "        # Protein embedding\n",
    "        # protein_inputs = self.esm_tokenizer(protein_sequence, return_tensors=\"pt\")\n",
    "        protein_inputs = {\n",
    "            \"input_ids\": protein_input_ids,\n",
    "            \"attention_mask\": protein_attention_mask\n",
    "        }\n",
    "        # perform in FP16 for lower memory usage (matmuls)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                protein_outputs = self.esm_model(**protein_inputs)\n",
    "        special_tokens_mask = (protein_inputs['input_ids'] == self.esm_tokenizer.cls_token_id)\\\n",
    "        | (protein_inputs['input_ids'] == self.esm_tokenizer.eos_token_id)\\\n",
    "        | (protein_inputs['input_ids'] == self.esm_tokenizer.pad_token_id)\n",
    "        valid_tokens_mask = ~special_tokens_mask\n",
    "        protein_embedding = (protein_outputs.last_hidden_state * valid_tokens_mask.unsqueeze(dim=-1)).mean(dim=1)\n",
    "        # protein_embedding = self.protein_projector(protein_embedding)  # Project to combined_embedding_dim\n",
    "        \n",
    "        # SMILES embedding\n",
    "        # chem_inputs = self.chem_tokenizer(ligand_smiles, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        chem_inputs = {\n",
    "            \"input_ids\": ligand_input_ids,\n",
    "            \"attention_mask\": ligand_attention_mask\n",
    "        }\n",
    "        special_tokens_mask = (chem_inputs['input_ids'] == self.chem_tokenizer.bos_token_id)\\\n",
    "        | (chem_inputs['input_ids'] == self.chem_tokenizer.eos_token_id)\\\n",
    "        | (chem_inputs['input_ids'] == self.chem_tokenizer.pad_token_id)\n",
    "        valid_tokens_mask = ~special_tokens_mask\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                chem_outputs = self.chem_model(**chem_inputs)\n",
    "        chem_embedding = (chem_outputs.last_hidden_state * valid_tokens_mask.unsqueeze(dim=-1)).mean(dim=1)\n",
    "        # chem_embedding = self.chem_projector(chem_embedding)  # Project to combined_embedding_dim\n",
    "        \n",
    "        # transform into common dimension\n",
    "        protein_attention = self.protein_linear(protein_embedding)\n",
    "        ligand_attention = self.ligand_linear(chem_embedding)\n",
    "\n",
    "        # compute attention used for both protein and ligand embeddings\n",
    "        combined_attention = F.relu(protein_attention + ligand_attention)\n",
    "        attention_scores = F.softmax(self.attention(combined_attention), dim=0)\n",
    "\n",
    "        # apply attention\n",
    "        attended_protein = protein_embedding * attention_scores  # Weighted protein embedding\n",
    "        attended_ligand = chem_embedding * attention_scores\n",
    "\n",
    "        # concatenate embeddings\n",
    "        combined = torch.cat([attended_protein, attended_ligand], dim=1)\n",
    "\n",
    "        # pass through hidden layer\n",
    "        combined_features = F.relu(self.combined_linear(combined))\n",
    "\n",
    "        # predict ic50 \n",
    "        ic50_prediction = self.output_layer(combined_features)\n",
    "\n",
    "        return ic50_prediction\n",
    "\n",
    "class BindingAffinityModelWithCrossAttention(nn.Module):\n",
    "    def __init__(self, esm_model_name, chem_model_name, combined_embedding_dim=512, attention_dim=256):\n",
    "        super().__init__()\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load pretrained ESM2 model for proteins\n",
    "        self.esm_model = AutoModel.from_pretrained(esm_model_name)\n",
    "        self.esm_tokenizer = AutoTokenizer.from_pretrained(esm_model_name)\n",
    "        self.esm_model.eval()\n",
    "        \n",
    "        # Load pretrained ChemLLM for SMILES (ligands)\n",
    "        self.chem_model = AutoModel.from_pretrained(chem_model_name)\n",
    "        self.chem_tokenizer = AutoTokenizer.from_pretrained(chem_model_name)\n",
    "        self.chem_model.eval()\n",
    "\n",
    "        # Disable gradient computation for both base models\n",
    "        for param in self.esm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.chem_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Protein and SMILES embedding dimensions\n",
    "        self.protein_embedding_dim = self.esm_model.config.hidden_size\n",
    "        self.chem_embedding_dim = self.chem_model.config.hidden_size\n",
    "        \n",
    "        # Protein queries ligand\n",
    "        self.protein_query = nn.Linear(self.protein_embedding_dim, attention_dim)\n",
    "        self.ligand_key = nn.Linear(self.chem_embedding_dim, attention_dim)\n",
    "        self.ligand_value = nn.Linear(self.chem_embedding_dim, attention_dim)\n",
    "\n",
    "        # Ligand queries protein\n",
    "        self.ligand_query = nn.Linear(self.chem_embedding_dim, attention_dim)\n",
    "        self.protein_key = nn.Linear(self.protein_embedding_dim, attention_dim)\n",
    "        self.protein_value = nn.Linear(self.protein_embedding_dim, attention_dim)\n",
    "\n",
    "        self.combined_linear = nn.Linear(self.protein_embedding_dim + self.chem_embedding_dim + 2 * attention_dim, combined_embedding_dim)\n",
    "        self.output_layer = nn.Linear(combined_embedding_dim, 1)\n",
    "\n",
    "    def cross_attention(self, key, query, value):\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / torch.sqrt(torch.tensor(key.size(-1), dtype=torch.float32))\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        return torch.matmul(attention_weights, value)\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            ligand_input_ids,\n",
    "            ligand_attention_mask,\n",
    "            protein_input_ids,\n",
    "            protein_attention_mask\n",
    "        ):\n",
    "        # Protein embedding\n",
    "        # protein_inputs = self.esm_tokenizer(protein_sequence, return_tensors=\"pt\")\n",
    "        protein_inputs = {\n",
    "            \"input_ids\": protein_input_ids,\n",
    "            \"attention_mask\": protein_attention_mask\n",
    "        }\n",
    "        # perform in FP16 for lower memory usage (matmuls)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                protein_outputs = self.esm_model(**protein_inputs)\n",
    "        special_tokens_mask = (protein_inputs['input_ids'] == self.esm_tokenizer.cls_token_id)\\\n",
    "        | (protein_inputs['input_ids'] == self.esm_tokenizer.eos_token_id)\\\n",
    "        | (protein_inputs['input_ids'] == self.esm_tokenizer.pad_token_id)\n",
    "        valid_tokens_mask = ~special_tokens_mask\n",
    "        protein_embedding = (protein_outputs.last_hidden_state * valid_tokens_mask.unsqueeze(dim=-1)).mean(dim=1)\n",
    "        # protein_embedding = self.protein_projector(protein_embedding)  # Project to combined_embedding_dim\n",
    "        \n",
    "        # SMILES embedding\n",
    "        # chem_inputs = self.chem_tokenizer(ligand_smiles, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        chem_inputs = {\n",
    "            \"input_ids\": ligand_input_ids,\n",
    "            \"attention_mask\": ligand_attention_mask\n",
    "        }\n",
    "        special_tokens_mask = (chem_inputs['input_ids'] == self.chem_tokenizer.bos_token_id)\\\n",
    "        | (chem_inputs['input_ids'] == self.chem_tokenizer.eos_token_id)\\\n",
    "        | (chem_inputs['input_ids'] == self.chem_tokenizer.pad_token_id)\n",
    "        valid_tokens_mask = ~special_tokens_mask\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                chem_outputs = self.chem_model(**chem_inputs)\n",
    "        chem_embedding = (chem_outputs.last_hidden_state * valid_tokens_mask.unsqueeze(dim=-1)).mean(dim=1)\n",
    "        # chem_embedding = self.chem_projector(chem_embedding)  # Project to combined_embedding_dim\n",
    "        \n",
    "        # cross attention protein queries ligand\n",
    "        prot_Q = self.protein_query(protein_embedding)\n",
    "        ligand_K = self.ligand_key(chem_embedding)\n",
    "        ligand_V = self.ligand_value(chem_embedding)\n",
    "        attended_ligand = self.cross_attention(prot_Q, ligand_K, ligand_V)\n",
    "\n",
    "        # cross attention ligand queries protein\n",
    "        ligand_Q = self.ligand_query(chem_embedding)\n",
    "        protein_K = self.protein_key(protein_embedding)\n",
    "        protein_V = self.protein_value(protein_embedding)\n",
    "        attended_protein = self.cross_attention(ligand_Q, protein_K, protein_V)\n",
    "\n",
    "        # Combine embeddings\n",
    "        combined = torch.cat([protein_embedding, chem_embedding, attended_ligand, attended_protein], dim=1)\n",
    "        combined_features = torch.relu(self.combined_linear(combined))\n",
    "        ic50_prediction = self.output_layer(combined_features)\n",
    "        return ic50_prediction\n",
    "\n",
    "# MultiHeadCrossAttention  \n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_heads=8, dropout=0.1, ffn_hidden_dim=2048):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.protein_to_ligand_attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ligand_to_protein_attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        ffn_hidden_dim = embed_dim * 3\n",
    "        self.ffn_protein = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_hidden_dim),\n",
    "            nn.ReLU(),  # Non-linear activation\n",
    "            nn.Linear(ffn_hidden_dim, embed_dim),\n",
    "        )\n",
    "        self.ffn_ligand = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_hidden_dim),\n",
    "            nn.ReLU(),  # Non-linear activation\n",
    "            nn.Linear(ffn_hidden_dim, embed_dim),\n",
    "        )\n",
    "        self.protein_norm = nn.LayerNorm(embed_dim)\n",
    "        self.ligand_norm = nn.LayerNorm(embed_dim)\n",
    "        self.ffn_protein_norm = nn.LayerNorm(embed_dim)\n",
    "        self.ffn_ligand_norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, protein_embedding, ligand_embedding, key_pad_mask_prot, key_pad_mask_ligand):\n",
    "        # Protein attending to ligand\n",
    "        attended_protein, _ = self.protein_to_ligand_attention(\n",
    "            query=protein_embedding, \n",
    "            key=ligand_embedding,\n",
    "            value=ligand_embedding,\n",
    "            key_padding_mask=key_pad_mask_ligand\n",
    "        )\n",
    "        attended_protein = self.protein_norm(protein_embedding + attended_protein)  # Residual connection\n",
    "        x_prot = self.ffn_protein(attended_protein)\n",
    "        x_prot = self.ffn_protein_norm(attended_protein + self.dropout(x_prot))\n",
    "\n",
    "        # Ligand attending to protein\n",
    "        attended_ligand, _ = self.ligand_to_protein_attention(\n",
    "            query=ligand_embedding, \n",
    "            key=protein_embedding, \n",
    "            value=protein_embedding,\n",
    "            key_padding_mask=key_pad_mask_prot\n",
    "        )\n",
    "        attended_ligand = self.ligand_norm(ligand_embedding + attended_ligand)  # Residual connection\n",
    "        x_ligand = self.ffn_ligand(attended_ligand)\n",
    "        x_ligand = self.ffn_ligand_norm(attended_ligand + self.dropout(x_ligand))\n",
    "        return x_prot, x_ligand\n",
    "\n",
    "class BindingAffinityModelWithMultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, esm_model_name, chem_model_name, num_layers=3, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load pretrained ESM2 model for proteins\n",
    "        self.esm_model = AutoModel.from_pretrained(esm_model_name)\n",
    "        self.esm_tokenizer = AutoTokenizer.from_pretrained(esm_model_name)\n",
    "        self.esm_model.eval()\n",
    "        \n",
    "        # Load pretrained ChemLLM for SMILES (ligands)\n",
    "        self.ligand_model = AutoModel.from_pretrained(chem_model_name)\n",
    "        self.ligand_tokenizer = AutoTokenizer.from_pretrained(chem_model_name)\n",
    "        self.ligand_model.eval()\n",
    "\n",
    "        # Disable gradient computation for both base models\n",
    "        for param in self.esm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.ligand_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Protein and SMILES embedding dimensions\n",
    "        self.protein_embedding_dim = self.esm_model.config.hidden_size\n",
    "        self.ligand_embedding_dim = self.ligand_model.config.hidden_size\n",
    "\n",
    "        self.project_to_common = nn.Linear(self.protein_embedding_dim, self.ligand_embedding_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            CrossAttentionLayer(embed_dim=self.ligand_embedding_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ffn_ic50 = nn.Sequential(\n",
    "            nn.Linear(2 * self.ligand_embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            ligand_input_ids,\n",
    "            ligand_attention_mask,\n",
    "            protein_input_ids,\n",
    "            protein_attention_mask\n",
    "        ):\n",
    "        # Protein embedding\n",
    "        # protein_inputs = self.esm_tokenizer(protein_sequence, return_tensors=\"pt\")\n",
    "        protein_inputs = {\n",
    "            \"input_ids\": protein_input_ids,\n",
    "            \"attention_mask\": protein_attention_mask\n",
    "        }\n",
    "        # perform in FP16 for lower memory usage (matmuls)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                protein_outputs = self.esm_model(**protein_inputs)\n",
    "        special_tokens_mask_prot = (protein_inputs['input_ids'] == self.esm_tokenizer.cls_token_id)\\\n",
    "        | (protein_inputs['input_ids'] == self.esm_tokenizer.eos_token_id)\\\n",
    "        | (protein_inputs['input_ids'] == self.esm_tokenizer.pad_token_id)\n",
    "        protein_embedding = protein_outputs.last_hidden_state\n",
    "        \n",
    "        # SMILES embedding\n",
    "        ligand_inputs = {\n",
    "            \"input_ids\": ligand_input_ids,\n",
    "            \"attention_mask\": ligand_attention_mask\n",
    "        }\n",
    "        special_tokens_mask_ligand = (ligand_inputs['input_ids'] == self.ligand_tokenizer.bos_token_id)\\\n",
    "        | (ligand_inputs['input_ids'] == self.ligand_tokenizer.eos_token_id)\\\n",
    "        | (ligand_inputs['input_ids'] == self.ligand_tokenizer.pad_token_id)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                ligand_outputs = self.ligand_model(**ligand_inputs)\n",
    "        ligand_embedding = ligand_outputs.last_hidden_state\n",
    "\n",
    "        # project embeddings to same dimension\n",
    "        protein_embedding = self.project_to_common(protein_embedding)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            protein_embedding, ligand_embedding = layer(protein_embedding, ligand_embedding, special_tokens_mask_prot, special_tokens_mask_ligand)\n",
    "\n",
    "        # Perform mean pooling\n",
    "        ligand_embedding = (ligand_embedding * ~special_tokens_mask_ligand.unsqueeze(dim=-1)).mean(dim=1)\n",
    "        protein_embedding = (protein_embedding * ~special_tokens_mask_prot.unsqueeze(dim=-1)).mean(dim=1)\n",
    "        # Combine embeddings\n",
    "        combined = torch.cat([protein_embedding, ligand_embedding], dim=1)\n",
    "        ic50_prediction = self.ffn_ic50(combined)\n",
    "        return ic50_prediction\n",
    "\n",
    "\n",
    "esm_model_name = \"facebook/esm2_t33_650M_UR50D\"  # Replace with the correct ESM2 model name\n",
    "chem_model_name = \"seyonec/ChemBERTa-zinc-base-v1\" # Replace with the correct ChemLLM model name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0398f180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum ligand length 698\n",
      "Minimum ligand length 8\n",
      "Mean ligand length 60.49\n",
      "Median ligand length 57.0\n",
      "Maximum protein length 7132\n",
      "Minimum protein length 9\n",
      "Mean protein length 733.63\n",
      "Median protein length 545.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum ligand length {df['ligand'].str.len().max()}\")\n",
    "print(f\"Minimum ligand length {df['ligand'].str.len().min()}\")\n",
    "print(f\"Mean ligand length {df['ligand'].str.len().mean():.2f}\")\n",
    "print(f\"Median ligand length {df['ligand'].str.len().median()}\")\n",
    "print(f\"Maximum protein length {df['protein'].str.len().max()}\")\n",
    "print(f\"Minimum protein length {df['protein'].str.len().min()}\")\n",
    "print(f\"Mean protein length {df['protein'].str.len().mean():.2f}\")\n",
    "print(f\"Median protein length {df['protein'].str.len().median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "366313b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9daafde8ba845c39b92b4ba58a7dc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4668d1ca3d546f9bc05502516777590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ligand', 'protein', 'ic50'],\n",
       "        num_rows: 3360\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ligand', 'protein', 'ic50'],\n",
       "        num_rows: 420\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ligand', 'protein', 'ic50'],\n",
       "        num_rows: 421\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data split \n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Filter protein sequences longer than 1024 and ligands longer than 512\n",
    "dataset = dataset.filter(lambda x: len(x[\"protein\"]) < 1024)\n",
    "dataset = dataset.filter(lambda x: len(x[\"ligand\"]) < 512)\n",
    "\n",
    "dataset_train_test = dataset.train_test_split(test_size=0.2)\n",
    "dataset_test_val = dataset_train_test[\"test\"].train_test_split(test_size=0.5)\n",
    "dataset_dict = {\n",
    "    \"train\": dataset_train_test['train'],\n",
    "    \"test\": dataset_test_val[\"train\"],\n",
    "    \"validation\": dataset_test_val['test']\n",
    "}\n",
    "dataset = DatasetDict(dataset_dict)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f35a59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chem tokenizer is fast: True\n",
      "esm tokenizer is fast: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb1ddd4eff5415f8ef585c1c42958d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18b798245164a399b1a4aa348dcafee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/420 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7472c5140b84133972fa5ca705801eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3deaaec27bbf484f807ffe1467b8d8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37171e802f54797bf11da55a7297131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/420 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54088f3b25424d8993b552078a74110b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ligand', 'protein', 'ic50', 'protein_input_ids', 'protein_attention_mask', 'ligand_input_ids', 'ligand_attention_mask'],\n",
       "        num_rows: 3360\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ligand', 'protein', 'ic50', 'protein_input_ids', 'protein_attention_mask', 'ligand_input_ids', 'ligand_attention_mask'],\n",
       "        num_rows: 420\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ligand', 'protein', 'ic50', 'protein_input_ids', 'protein_attention_mask', 'ligand_input_ids', 'ligand_attention_mask'],\n",
       "        num_rows: 421\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization of ligands and protein sequences\n",
    "chem_tokenizer = AutoTokenizer.from_pretrained(chem_model_name)\n",
    "esm_tokenizer = AutoTokenizer.from_pretrained(esm_model_name)\n",
    "print(f\"chem tokenizer is fast: {chem_tokenizer.is_fast}\")\n",
    "print(f\"esm tokenizer is fast: {esm_tokenizer.is_fast}\")\n",
    "\n",
    "def tokenize_ligands(examples):\n",
    "    toks = chem_tokenizer(examples[\"ligand\"], truncation=True)\n",
    "    return {\n",
    "        \"ligand_input_ids\": toks[\"input_ids\"],\n",
    "        \"ligand_attention_mask\": toks[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "def tokenize_proteins(examples):\n",
    "    toks =  esm_tokenizer(examples[\"protein\"], truncation=True)\n",
    "    return {\n",
    "        \"protein_input_ids\": toks[\"input_ids\"],\n",
    "        \"protein_attention_mask\": toks[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_proteins, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_ligands, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63287520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom data collator\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "chem_collator = DataCollatorWithPadding(tokenizer=chem_tokenizer)\n",
    "esm_collator = DataCollatorWithPadding(tokenizer=esm_tokenizer)\n",
    "\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, chem_collator, esm_collator):\n",
    "            self.chem_collator = chem_collator\n",
    "            self.esm_collator = esm_collator\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch_ligand = [{\"input_ids\": b[\"ligand_input_ids\"], \"attention_mask\": b[\"ligand_attention_mask\"]} for b in batch]\n",
    "        batch_protein = [{\"input_ids\": b[\"protein_input_ids\"], \"attention_mask\": b[\"protein_attention_mask\"]} for b in batch]\n",
    "\n",
    "        collated_chem = self.chem_collator(batch_ligand)\n",
    "        collated_esm = self.esm_collator(batch_protein)\n",
    "\n",
    "        return {\n",
    "            \"ligand_input_ids\": collated_chem[\"input_ids\"],\n",
    "            \"ligand_attention_mask\": collated_chem[\"attention_mask\"],\n",
    "            \"protein_input_ids\": collated_esm[\"input_ids\"],\n",
    "            \"protein_attention_mask\": collated_esm[\"attention_mask\"],\n",
    "            \"ic50\": torch.tensor([x['ic50'] for x in batch])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43bd8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bs = 32\n",
    "collator = CustomDataCollator(chem_collator=chem_collator, esm_collator=esm_collator)\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=bs, collate_fn=collator)\n",
    "test_dataloader = DataLoader(tokenized_dataset[\"test\"], batch_size=bs, collate_fn=collator)\n",
    "val_dataloader = DataLoader(tokenized_dataset[\"validation\"], batch_size=bs, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbe18464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TESTING MEMORY WITH SINGLE BATCH\n",
    "# batch = next(iter(train_dataloader))\n",
    "# batch[\"ligand_input_ids\"]\n",
    "# chem_inputs = {\n",
    "#             \"input_ids\": batch[\"ligand_input_ids\"].to(device),\n",
    "#             \"attention_mask\": batch[\"ligand_attention_mask\"].to(device)\n",
    "#         }\n",
    "# prot_inputs = {\n",
    "#             \"input_ids\": batch[\"protein_input_ids\"].to(device),\n",
    "#             \"attention_mask\": batch[\"protein_attention_mask\"].to(device)\n",
    "#         }\n",
    "# o = esm_model(**prot_inputs)\n",
    "# o['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ce22cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Training loop \n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "WARMUP_STEPS = 200\n",
    "EPOCHS = 1\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < WARMUP_STEPS:\n",
    "        # Linear warmup\n",
    "        return step / WARMUP_STEPS\n",
    "    else:\n",
    "        remaining_steps = total_steps - WARMUP_STEPS\n",
    "        decay_step = step - WARMUP_STEPS\n",
    "        return max(0.0, 1.0 - decay_step / remaining_steps)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "model = BindingAffinityModelWithMultiHeadCrossAttention(esm_model_name, chem_model_name).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=0)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "total_steps = EPOCHS * len(train_dataloader)\n",
    "\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader):\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch: {epoch + 1}/{EPOCHS}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_progress = tqdm(train_dataloader, desc=\"Training\")\n",
    "\n",
    "        for batch in train_progress:\n",
    "            ligand_input_ids = batch[\"ligand_input_ids\"].to(device)\n",
    "            ligand_attention_mask = batch[\"ligand_attention_mask\"].to(device)\n",
    "            protein_input_ids = batch[\"protein_input_ids\"].to(device)\n",
    "            protein_attention_mask = batch[\"protein_attention_mask\"].to(device)\n",
    "            targets = batch[\"ic50\"].unsqueeze(dim=-1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(ligand_input_ids, ligand_attention_mask, protein_input_ids, protein_attention_mask)\n",
    "            loss = criterion(preds, targets)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            step += 1\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                wandb.log({\"train_loss\": loss.item()})\n",
    "                wandb.log({\"lr\": optimizer.param_groups[0][\"lr\"]})\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        print(f\"Epoch: {epoch} Train loss: {train_loss}\")\n",
    "\n",
    "        step = 0\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_progress = tqdm(val_dataloader, desc=\"Validation\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_progress:\n",
    "                ligand_input_ids = batch[\"ligand_input_ids\"].to(device)\n",
    "                ligand_attention_mask = batch[\"ligand_attention_mask\"].to(device)\n",
    "                protein_input_ids = batch[\"protein_input_ids\"].to(device)\n",
    "                protein_attention_mask = batch[\"protein_attention_mask\"].to(device)\n",
    "                targets = batch[\"ic50\"].unsqueeze(dim=-1).to(device)\n",
    "                preds = model(ligand_input_ids, ligand_attention_mask, protein_input_ids, protein_attention_mask)\n",
    "                loss = criterion(preds, targets)\n",
    "                val_loss += loss.item()\n",
    "                step += 1\n",
    "                if step % 10 == 0:\n",
    "                    wandb.log({\"val_loss\": loss.item()})\n",
    "                \n",
    "        val_loss /= len(val_dataloader)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch: {epoch} Val loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecc7ce6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105/105 [04:34<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train loss: 19.98928334372384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:27<00:00,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Val loss: 14.828462055751256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57cdca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"affinity.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72a79741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test set \n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    test_progress = tqdm(test_loader, desc=\"Test set\")\n",
    "    with torch.no_grad():\n",
    "        for batch in test_progress:\n",
    "            ligand_input_ids = batch[\"ligand_input_ids\"].to(device)\n",
    "            ligand_attention_mask = batch[\"ligand_attention_mask\"].to(device)\n",
    "            protein_input_ids = batch[\"protein_input_ids\"].to(device)\n",
    "            protein_attention_mask = batch[\"protein_attention_mask\"].to(device)\n",
    "            targets = batch[\"ic50\"].unsqueeze(dim=-1).to(device)\n",
    "            preds = model(ligand_input_ids, ligand_attention_mask, protein_input_ids, protein_attention_mask)\n",
    "            all_targets.append(targets)\n",
    "            all_predictions.append(preds)\n",
    "\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "\n",
    "    loss = torch.nn.MSELoss()\n",
    "    print(f\"Test set mean squared error: {loss(all_predictions, all_targets)}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d5b8659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:27<00:00,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set mean squared error: 13.255571365356445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader=test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
